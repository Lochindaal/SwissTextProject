\subsection{Environment or Setup?}
We train the model on a single XYZ GPU (XYG memory) for 75 epochs, with a batch size of 24, a dropout probability of 0.25, and a learning rate of 0.0001. We perform minimal tuning of our model's hyperparameters following the work
of \newcite{Agarwal2020LTLUDEAL} as they based some of their work on \newcite{pluss2020}, which achieved the best results in previous challenges, and incorporates data augmentation.
.
